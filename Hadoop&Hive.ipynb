{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cca7e2",
   "metadata": {},
   "source": [
    "# Hadoop"
   ]
  },
  {
   "cell_type": "raw",
   "id": "deee325d",
   "metadata": {},
   "source": [
    "ce = cloudera \n",
    "cem = ce manager\n",
    "command = cmd\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93d34fd5",
   "metadata": {},
   "source": [
    "welcome page of cloudeera(ce)\n",
    "can see node manager address\n",
    "command system => linux\n",
    "to on ce we require to start ce manager\n",
    "for that launch ce express\n",
    "2 versions = express(free) and enterprise(paid) nad after ce 6.0 paid\n",
    "now start ce manager\n",
    "u can see hadoop cluster"
   ]
  },
  {
   "cell_type": "raw",
   "id": "233eb3c0",
   "metadata": {},
   "source": [
    "ce manager = yarn = can see log files and services"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1fd4574",
   "metadata": {},
   "source": [
    "can see the available services from add a service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4475d0",
   "metadata": {},
   "source": [
    "# HDFS Commands"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13a8df3e",
   "metadata": {},
   "source": [
    "cntrl l = clear screen\n",
    "ls\n",
    "\n",
    "to give any command in hadoop system = start with =  hdfs dfs \n",
    "or = hadoop fs\n",
    "hdfs dfs -ls /\n",
    "\n",
    "pwd\n",
    "whoami\n",
    "mkdir firstdir\n",
    "cd firstdir\n",
    "cd .. = to go to the previous dir\n",
    "touch firstfile.txt = to create empty file\n",
    "cat firstfile.txt = to check content\n",
    "\n",
    "vi firstfile.txt = to edit the file if file doesnt exit it creates\n",
    "press i to write\n",
    "to save => esc => :wq! => enter\n",
    "cat firstfile.txt\n",
    "to copy => cp firstdir/firstfile <path of 2nd dir>\n",
    "\n",
    "gedit firstfile.txt = to edit file\n",
    "\n",
    "hdfs dfs -mkdir FirstDir = to make dir in hadoop cluster\n",
    "\n",
    "to put file from linux to hadoop\n",
    "hdfs dfs -put firstfile.txt /user/cloudera/FirstDir\n",
    "hdfs dfs -cat /user/cloudera/FirstDir/firstfile.txt = to check\n",
    "\n",
    "for hadoop to linux\n",
    "same just instead of -put = -get\n",
    "\n",
    "to check usage of any command\n",
    "hdfs dfs -usage ls\n",
    "hdfs dfs -help ls\n",
    "\n",
    "for removing =\n",
    "hdfs dfs -rm\n",
    "but it will not remove directly => will go in trash folder => if removing for the 1st time it will create that trash folder\n",
    "\n",
    "to check stat of file in hadoop\n",
    "hdfs dfs -stat /user\n",
    "\n",
    "to check size of file in dir\n",
    "hdfs dfs -du /user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6e2ea",
   "metadata": {},
   "source": [
    "# mapper code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5138165d",
   "metadata": {},
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"mapper.py\"\"\"\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "\tline = line.strip()\n",
    "\twords = line.split()\n",
    "\tfor word in words:\n",
    "\t\tprint '%s\\t%s' % (word, 1)\n",
    "        \n",
    "# counting the frequency of words basically"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8060e11",
   "metadata": {},
   "source": [
    "#!/usr/bin/python: This is called a shebang line. It indicates the path to the Python interpreter that should be used to execute the script. In this case, it specifies that the system should use the Python interpreter located at /usr/bin/python."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a16fd6c1",
   "metadata": {},
   "source": [
    "import sys: This line imports the sys module, which provides access to some variables used or maintained by the Python interpreter and functions that interact strongly with the interpreter.\n",
    "\n",
    "for line in sys.stdin:: This initiates a loop that iterates over each line of input received from the standard input (stdin). The input is read line by line.\n",
    "\n",
    "line = line.strip(): This removes leading and trailing whitespaces from the input line.\n",
    "\n",
    "words = line.split(): This splits the line into a list of words based on whitespace. It uses the split method without any arguments, which means it splits the string at spaces.\n",
    "\n",
    "for word in words:: This initiates another loop that iterates over each word in the list of words.\n",
    "\n",
    "print '%s\\t%s' % (word, 1): This prints the word and the number 1, separated by a tab character (\\t). The output of this script is typically sent to the standard output (stdout) and is then used as input for the reducer in a MapReduce job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed023c4",
   "metadata": {},
   "source": [
    "# Reducer code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a859ca",
   "metadata": {},
   "source": [
    "7. gedit reducer.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"reducer.py\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17d504",
   "metadata": {},
   "source": [
    "# doing it on hadoop cluster"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5af6c1d7",
   "metadata": {},
   "source": [
    "we will use the streaming jar\n",
    "\n",
    "move input file to hdfs\n",
    "\n",
    "create a dir\n",
    "hdfs dfs -mkdir aditya\n",
    "\n",
    "copy the file from local to hdfs \n",
    "hdfs dfs -copyFromLocal -f /home/cloudera/word.txt TESTONE/\n",
    "\n",
    "now to run python file to hadoop we use hadoop streaming\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84e50852",
   "metadata": {},
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming*.jar \\\n",
    "  -files mapper.py,reducer.py \\\n",
    "  -mapper \"python mapper.py\" \\\n",
    "  -reducer \"python reducer.py\" \\\n",
    "  -input TESTONE/word.txt \\\n",
    "  -output TESTONE/output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e542a78",
   "metadata": {},
   "source": [
    "# View the output on HDFS\n",
    "hdfs dfs -cat /user/cloudera/TESTONE/output/part-*\n",
    "\n",
    "# Download the output to the local file system\n",
    "hdfs dfs -copyToLocal /user/cloudera/TESTONE/output /home/cloudera/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0eb5d",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5696f39d",
   "metadata": {},
   "source": [
    "\n",
    "Step 2: Create a Database\n",
    "# Create a Database\n",
    "CREATE DATABASE IF NOT EXISTS mydatabase;\n",
    "\n",
    "Step 3: Switch to the Database\n",
    "-- Switch to the newly created database\n",
    "USE mydatabase;\n",
    "\n",
    "Step 4: Create an Internal Hive Table\n",
    "# Create an internal Hive table. Internal tables are managed by Hive, and the data resides in Hive's warehouse directory.\n",
    "-- Create an internal table\n",
    "CREATE TABLE internal_table (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    ");\n",
    "\n",
    "Step 5: Load Data into the Internal Table\n",
    "-- Load data into the internal table\n",
    "INSERT INTO TABLE internal_table VALUES (1, 'John', 25), (2, 'Alice', 30), (3, 'Bob', 28);\n",
    "\n",
    "Step 6: Query the Internal Table\n",
    "-- Query the internal table\n",
    "SELECT * FROM internal_table;\n",
    "\n",
    "-- Filter data based on age\n",
    "SELECT * FROM internal_table WHERE age > 25;\n",
    "\n",
    "-- Calculate average age\n",
    "SELECT AVG(age) as average_age FROM internal_table;\n",
    "\n",
    "-- Sort data by age in descending order\n",
    "SELECT * FROM internal_table ORDER BY age DESC;\n",
    "\n",
    "Step 7: Create an External Hive Table\n",
    "#Create an external Hive table. External tables are used when you want Hive to reference data stored outside of Hive's warehouse directory.\n",
    "-- Create an external table\n",
    "CREATE EXTERNAL TABLE external_table (\n",
    "    id INT,\n",
    "    course STRING,\n",
    "    instructor STRING\n",
    ") LOCATION '/user/cloudera/external_data';\n",
    "\n",
    "# before external table create the directory first\n",
    "hdfs dfs -mkdir -p /user/cloudera/external_data\n",
    "# grant permission if necessary\n",
    "hdfs dfs -chmod -R 777 /user/cloudera/external_data\n",
    "\n",
    "\n",
    "# Place a csv file with sample data into the specified location (/user/cloudera/external_data in this case).\n",
    "\n",
    "#for this \n",
    "gedit /home/cloudera/sample_data.csv\n",
    "# 1   ComputerScience   Amit\n",
    "2   Physics           Raj\n",
    "3   History           Natasha\n",
    "4   Mathematics       Prakhar\n",
    "\n",
    "# copy it to hdfs\n",
    "hdfs dfs -copyFromLocal sample_data.txt /user/cloudera/external_data\n",
    "# to verify\n",
    "hdfs dfs -ls /user/cloudera/external_data\n",
    "\n",
    "Step 8: Load Data into the External Table\n",
    "-- Load data into the external table\n",
    "LOAD DATA INPATH '/user/cloudera/external_data/sample_data.txt' INTO TABLE external_table;\n",
    "\n",
    "Step 9: Join Internal and External Tables\n",
    "-- Join internal and external tables\n",
    "SELECT internal_table.id, internal_table.name, internal_table.age, external_table.course\n",
    "FROM internal_table\n",
    "JOIN external_table ON internal_table.id = external_table.id;\n",
    "# for more in gpt\n",
    "\n",
    "Step 10: Create an Index\n",
    "-- Create an index\n",
    "CREATE INDEX idx_age ON TABLE internal_table (age) AS 'COMPACT' WITH DEFERRED REBUILD;\n",
    "Expected Output: No output, but the index idx_age is created on the age column in internal_table.\n",
    "\n",
    "Step 11: Rebuild the Index\n",
    "-- Rebuild the index\n",
    "ALTER INDEX idx_age ON internal_table REBUILD;\n",
    "Expected Output: No output, but the index idx_age is rebuilt on the age column in internal_table.\n",
    "\n",
    "\n",
    "\n",
    "Step 12: Drop Tables and Index\n",
    "-- Drop internal table\n",
    "DROP TABLE internal_table;\n",
    "\n",
    "-- Drop external table\n",
    "DROP TABLE external_table;\n",
    "\n",
    "-- Drop index\n",
    "DROP INDEX idx_age ON internal_table;\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17672c28",
   "metadata": {},
   "source": [
    "# OR for EXTERNAL table can refer \n",
    "https://phoenixnap.com/kb/hive-create-external-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefece3",
   "metadata": {},
   "source": [
    "# Hive in data mining"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3354a8f",
   "metadata": {},
   "source": [
    "While Hive itself is not a data mining tool, it plays a significant role in the big data ecosystem, and it can be used in conjunction with other tools and techniques, including data mining approaches."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bff851a",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "Suppose you have a CSV file named employee_data.csv in HDFS, and you want to create a Hive table for analysis.\n",
    "\n",
    "hdfs dfs -put employee_data.csv /user/hive/warehouse/\n",
    "\n",
    "-- Create a Hive table\n",
    "CREATE TABLE employee (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    department STRING,\n",
    "    salary INT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "-- Load data into the Hive table\n",
    "LOAD DATA INPATH '/user/hive/warehouse/employee_data.csv' INTO TABLE employee;\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "302d066d",
   "metadata": {},
   "source": [
    "2. Data Exploration\n",
    "\n",
    "-- Show the first few rows\n",
    "SELECT * FROM employee LIMIT 5;\n",
    "\n",
    "-- Calculate average salary by department\n",
    "SELECT department, AVG(salary) AS avg_salary\n",
    "FROM employee\n",
    "GROUP BY department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc175b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
